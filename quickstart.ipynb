{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luSS6e7lSL0H"
   },
   "source": [
    "# UNETR-2D for mitochondria segmentation in 2D\n",
    "    \n",
    "     Author: Aitor Gonz√°lez\n",
    "---\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://www.epfl.ch/labs/cvlab/wp-target/uploads/2018/08/FIBSLICE0035_left_top.png\" alt=\"EM image\" width=\"256\">\n",
    "<img src=\"https://www.epfl.ch/labs/cvlab/wp-target/uploads/2018/08/masks_FIBSLICE0035_left_top.png\" alt=\"Binary label image\" width=\"256\">\n",
    "<figcaption>Input image (left) and corresponding semantic segmentation (right)</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ew3GyS-l_d2"
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if repository is not downloaded (common if used colab link)\n",
    "if not os.path.exists('./utils') and not os.path.exists('./models'):\n",
    "    !git clone https://github.com/AAitorG/UNETR_2D.git UNETR_2D\n",
    "    !python3 -m pip install -r UNETR_2D/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from models import *\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import gc\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "set_gpu(gpu_id=0)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gzC_kusj-PI"
   },
   "source": [
    "We download and unzip the dataset (it might take a few minutes but later it is faster than having the data directly as images in Drive):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tdpnwFVCj_Fh",
    "outputId": "6a64e543-5807-4b08-8cbf-59a4ee83a9a8"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./Data'):\n",
    "    # make a copy of the \n",
    "    # Unzip FIBSEM (EPFL) dataset\n",
    "    !wget 'https://ehubox.ehu.eus/s/QnCc4L4ZYpHpB2s/download'\n",
    "    !unzip -q download\n",
    "    !rm download\n",
    "    !mkdir Data\n",
    "    !mv data Data/Lucchi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SK-GgwqbrkI2"
   },
   "source": [
    "Now we should be able to read the list of **165 training images** and their corresponding 165 label images, that are organized as follows:\n",
    "\n",
    "```\n",
    "data/\n",
    "    |-- train/\n",
    "    |    |-- x/\n",
    "    |    |      training-0001.tif\n",
    "    |    |      ...\n",
    "    |    |-- y/\n",
    "    |    |      training_groundtruth-0001.tif\n",
    "    |    |        ...\n",
    "    |-- test/\n",
    "    |    |-- x/\n",
    "    |    |      testing-0001.tif\n",
    "    |    |      ...\n",
    "    |    |-- y/\n",
    "    |    |      testing_groundtruth-0001.tif\n",
    "    |    |      ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utEi1a3QXG0r"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-VccRfwXG0s"
   },
   "outputs": [],
   "source": [
    "data_path = './Data/'\n",
    "# Training dataset\n",
    "train_datasets = 'Lucchi'\n",
    "# Test dataset\n",
    "test_datasets = 'Lucchi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpERsKL_L6Q4"
   },
   "outputs": [],
   "source": [
    "## === Training parameters ===\n",
    "# number of epochs\n",
    "numEpochs = 10\n",
    "# patience (if (patience <= 0): patience will not be used)\n",
    "patience = 30\n",
    "# learning rate\n",
    "lr = 1e-4\n",
    "# weight_decay  (for AdamW)\n",
    "wd = 1e-5\n",
    "# Scheduler: 'oneCycle', 'reduce', 'cosine',  None\n",
    "schedule = 'oneCycle'\n",
    "# Optimizer name: 'Adam', 'SGD', 'rmsprop', 'AdamW'\n",
    "optimizer_name = 'AdamW'\n",
    "# Loss function name: 'bce', 'bce_dice', 'mse'\n",
    "loss_acronym = 'bce'\n",
    "# batch size\n",
    "batch_size_value = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSggco_zL7lo"
   },
   "outputs": [],
   "source": [
    "## === Network parameters ===\n",
    "# Network architecture: UNETR_2D, YNETR_2D,\n",
    "model_name = 'UNETR_2D'\n",
    "# initial filters (16 x num_channels)\n",
    "num_filters = 32\n",
    "# conv kernel initializer: 'glorot_uniform', 'he_normal'\n",
    "kernel_init = 'he_normal'\n",
    "# conv part activation function\n",
    "activation = 'relu'\n",
    "# patch size\n",
    "patch_size = 16\n",
    "# hidden dimension\n",
    "hidden_dim = 64\n",
    "# number of transformer encoders\n",
    "transformer_layers = 4\n",
    "# number of heads per MHA module\n",
    "num_heads = 4\n",
    "# transformer mlp dimentions\n",
    "mlp_dim = [256, 64]\n",
    "# number of output channels (number of classes)\n",
    "out_channels = 1\n",
    "# denoise type: cutout, gaussNoise, coarseSaltP, emulate_LR, gaussian_filter, defocusBlur, motionBlur, pixeldropout\n",
    "posible_dataAug = []\n",
    "# dropout value # [0.1, 0.1, 0.2, 0.2, 0.3] (list is also allowed)\n",
    "dropout = 0.0\n",
    "# multiple of ViT layers that will be used for each skip connection (Sm)\n",
    "ViT_hidd_mult_skipC = 1\n",
    "# Use Batch Normalization layers\n",
    "batch_norm = True\n",
    "# Use Data Augmentation\n",
    "da = True\n",
    "# tensorflow additional data augmentation layers (use tf layer, if multiple layers, then use sequential() and add them)\n",
    "extra_tf_data_augmentation = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25ZYiZwppTu0"
   },
   "outputs": [],
   "source": [
    "# === Extra parameters ===\n",
    "# Load weights for FineTunning\n",
    "use_saved_model = False\n",
    "# Path to save weights (After training)\n",
    "out_dir = './model_weights'\n",
    "# Path to save plots\n",
    "img_out_dir = './plots'\n",
    "# filenames for trained model weights (h5)\n",
    "weights_filename = out_dir + '/weights-{}-src-{}-bce-nf-{}-bs-{}-{}-{}.weights.h5'.format(\n",
    "                                                    model_name, train_datasets, num_filters, batch_size_value,\n",
    "                                                    optimizer_name, 'None' if schedule is None else schedule )\n",
    "# Weights file path (weights that will be loaded if use_saved_model is TRUE)\n",
    "model_path = ''\n",
    "\n",
    "# image input size (this does not change the data size!)\n",
    "input_shape = (256,256,1)\n",
    "# number of random patches (with number lower than 0, sequential patches will be used)\n",
    "n_patches = -1\n",
    "\n",
    "# evaluation parameters\n",
    "# patch size\n",
    "patch_h, patch_w = (256,256)\n",
    "# relevant patch size\n",
    "relevant_h, relevant_w = (128,128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dif7qeJnQTLY"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2nm_cB9ZQWDc",
    "outputId": "ac31d9df-62a2-4dfc-8c0c-6e08e4078bb3"
   },
   "outputs": [],
   "source": [
    "h_cuts = 0\n",
    "v_cuts = 0\n",
    "\n",
    "source_path = os.path.join(data_path, train_datasets, 'train')\n",
    "ds_imgs, ds_lbls = get_xy_image_list(source_path)\n",
    "\n",
    "assert len(ds_imgs) > 0, 'There in NO data, check path: {}.'.format(source_path)\n",
    "assert len(ds_imgs) == len(ds_lbls), 'There is different ammount of images and labels. Images: {}  Labels: {}'.format(len(ds_imgs), len(ds_lbls))\n",
    "\n",
    "h, w = ds_imgs[0].shape\n",
    "exp_h, exp_w, _ = input_shape\n",
    "\n",
    "h_cuts = int(np.ceil(w/exp_w))\n",
    "v_cuts = int(np.ceil(h/exp_h))\n",
    "\n",
    "#print(\"h_cuts: {} \\t v_cuts: {}\".format(h_cuts, v_cuts))\n",
    "\n",
    "if w%exp_w != 0 and h%exp_h != 0:\n",
    "    w_parts = w/exp_w\n",
    "    h_parts = h/exp_h\n",
    "    new_w = int(np.ceil(w_parts))*exp_w\n",
    "    new_h = int(np.ceil(h_parts))*exp_h\n",
    "    # MIRROR PADDING\n",
    "    ds_imgs = [mirror_border(x, new_h, new_w) for x in ds_imgs]\n",
    "    ds_lbls = [mirror_border(x, new_h, new_w) for x in ds_lbls]\n",
    "    # ZERO PADDING (for 256x256 patches by default)\n",
    "    #ds_imgs = [add_padding(x) for x in ds_imgs]\n",
    "    #ds_lbls = [add_padding(x) for x in ds_lbls]\n",
    "\n",
    "if n_patches < 0:\n",
    "    # sequential patches\n",
    "    ds_imgs = create_patches( ds_imgs, h_cuts, v_cuts )\n",
    "    ds_lbls = create_patches( ds_lbls, h_cuts, v_cuts )\n",
    "\n",
    "    #ds_imgs, ds_lbls = filter_patches(ds_imgs, ds_lbls)\n",
    "else:\n",
    "    # random patches\n",
    "    p_ds_imgs = []\n",
    "    p_ds_lbls = []\n",
    "    while len(p_ds_imgs)<n_patches:\n",
    "        a,b = create_random_patches( ds_imgs, ds_lbls, 1, [256, 256] )\n",
    "        #a, b = filter_patches(a, b)\n",
    "        p_ds_imgs = p_ds_imgs + a\n",
    "        p_ds_lbls = p_ds_lbls + b\n",
    "    ds_imgs = p_ds_imgs[:n_patches]\n",
    "    ds_lbls = p_ds_lbls[:n_patches]\n",
    "\n",
    "ds_imgs = np.expand_dims(ds_imgs, axis=-1)\n",
    "ds_lbls = np.expand_dims(ds_lbls, axis=-1)\n",
    "\n",
    "input_images = np.array(ds_imgs)\n",
    "gt_labels = np.array(ds_lbls)\n",
    "\n",
    "train_data_size = input_images.shape[0] * 0.9\n",
    "val_data_size = input_images.shape[0] * 0.1\n",
    "\n",
    "print('\\n Data shape:',input_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6gzJuBOB5Sn"
   },
   "outputs": [],
   "source": [
    "# get crappify function\n",
    "crappify = get_crappify(posible_dataAug) \n",
    "# Get generator\n",
    "train_generator, val_generator = get_train_val_generators(  X_data = input_images,\n",
    "                                                            Y_data = input_images if loss_acronym == 'mse' else gt_labels,\n",
    "                                                            validation_split = 0.1,\n",
    "                                                            rescale = 1./255,\n",
    "                                                            horizontal_flip=True if da else False,\n",
    "                                                            vertical_flip=True if da else False,\n",
    "                                                            rotation_range = 180 if da else 0,\n",
    "                                                            #width_shift_range=0.2,\n",
    "                                                            #height_shift_range=0.2,\n",
    "                                                            #shear_range=0.2,\n",
    "                                                            batch_size=batch_size_value,\n",
    "                                                            show_examples=False,\n",
    "                                                            preprocessing_function = crappify if loss_acronym == 'mse' else None,\n",
    "                                                            val_preprocessing_function = crappify if loss_acronym == 'mse' else None, )\n",
    "train_generator = (pair for pair in train_generator)\n",
    "val_generator = (pair for pair in val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "oWBHGcTe_E6-",
    "outputId": "ee6311bb-147f-46c7-fd24-228f96a3f926"
   },
   "outputs": [],
   "source": [
    "for x, y in train_generator:\n",
    "    display([x[0,:,:,0],y[0,:,:,0]])\n",
    "    break\n",
    "for x, y in val_generator:\n",
    "    display([x[0,:,:,0],y[0,:,:,0]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91y9sVITNed1"
   },
   "source": [
    "## Compile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3HWEthr_L5kn",
    "outputId": "59193731-13f0-4294-fd2f-23b677cfe9ae"
   },
   "outputs": [],
   "source": [
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "clear_session()\n",
    "gc.collect()\n",
    "\n",
    "### CALLBACKS ###\n",
    "callbacks = []\n",
    "\n",
    "if patience > 0:\n",
    "    # callback for early stop\n",
    "    earlystopper = EarlyStopping(patience=patience, verbose=1, restore_best_weights=True)\n",
    "    callbacks.append(earlystopper)\n",
    "\n",
    "if schedule == 'oneCycle':\n",
    "    # callback for one-cycle schedule\n",
    "    steps = np.ceil(train_data_size / batch_size_value) * numEpochs\n",
    "    #steps = np.ceil(len(X_train) / batch_size_value) * numEpochs\n",
    "    lr_schedule = OneCycleScheduler(lr, steps)\n",
    "    callbacks.append(lr_schedule)\n",
    "elif schedule == 'reduce':\n",
    "    # callback to reduce the learning rate in the plateau\n",
    "    lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                            patience=patience, min_lr=(lr/10))\n",
    "    callbacks.append(lr_schedule)\n",
    "elif schedule == 'cosine':\n",
    "    # this scheduler is not a callback\n",
    "    steps = np.ceil(train_data_size / batch_size_value) * numEpochs\n",
    "    lr = tf.keras.optimizers.schedules.CosineDecay(lr, steps)  \n",
    "\n",
    "\n",
    "# create the network and compile it with its optimizer\n",
    "if model_name == 'UNETR_2D':\n",
    "    model = UNETR_2D(\n",
    "            input_shape = input_shape,\n",
    "            patch_size = patch_size,\n",
    "            num_patches = (input_shape[0]**2)//(patch_size**2),\n",
    "            projection_dim = hidden_dim,\n",
    "            transformer_layers = transformer_layers,\n",
    "            num_heads = num_heads,\n",
    "            transformer_units = mlp_dim, \n",
    "            data_augmentation = extra_tf_data_augmentation,\n",
    "            num_filters = num_filters,\n",
    "            num_classes = out_channels,\n",
    "            decoder_activation = activation,\n",
    "            decoder_kernel_init = kernel_init,\n",
    "            ViT_hidd_mult=ViT_hidd_mult_skipC,\n",
    "            batch_norm = batch_norm,\n",
    "            dropout = dropout,\n",
    "        )\n",
    "elif model_name == 'YNETR_2D':   \n",
    "    model = YNETR_2D(\n",
    "                input_shape = input_shape,\n",
    "                patch_size = patch_size,\n",
    "                num_patches = (input_shape[0]**2)//(patch_size**2),\n",
    "                projection_dim = hidden_dim,\n",
    "                transformer_layers = transformer_layers,\n",
    "                num_heads = num_heads,\n",
    "                transformer_units = mlp_dim, \n",
    "                data_augmentation = extra_tf_data_augmentation,\n",
    "                num_filters = num_filters, \n",
    "                num_classes = out_channels,\n",
    "                activation = activation,\n",
    "                kernel_init = kernel_init,\n",
    "                ViT_hidd_mult=ViT_hidd_mult_skipC,\n",
    "                batch_norm = batch_norm,\n",
    "                dropout = dropout,\n",
    "            )\n",
    "        \n",
    "\n",
    "if optimizer_name == 'SGD':\n",
    "    optim =  tf.keras.optimizers.SGD(\n",
    "            lr=lr, momentum=0.99, decay=0.0, nesterov=False)\n",
    "elif optimizer_name == 'Adam':\n",
    "    optim = tf.keras.optimizers.Adam( learning_rate=lr )\n",
    "elif optimizer_name == 'rmsprop':\n",
    "    optim = tf.keras.optimizers.RMSprop( learning_rate=lr )\n",
    "elif optimizer_name == 'AdamW':\n",
    "    optim = tf.keras.optimizers.AdamW( weight_decay = wd, learning_rate=lr )\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if loss_acronym == 'bce':\n",
    "    loss_funct = 'binary_crossentropy'\n",
    "elif loss_acronym == 'bce_dice':\n",
    "    loss_funct = bce_dice_loss\n",
    "elif loss_acronym == 'mse': # dont change this acronym (is used to know when is training for denoising)\n",
    "    loss_funct = 'mean_squared_error'\n",
    "\n",
    "if loss_acronym == 'mse':\n",
    "    eval_metric = [psnr, ssim]\n",
    "else:\n",
    "    eval_metric = [jaccard_index]\n",
    "                \n",
    "# compile the model with the specific optimizer, loss function and metric\n",
    "model.compile(optimizer=optim, loss=loss_funct, metrics=eval_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wD0M6N1wR5S"
   },
   "source": [
    "### show model architecture\n",
    "Have a quick look at the resulting model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "zy9IiPaDwR5W",
    "outputId": "38bf6b83-49e9-4dc8-9bd4-4b1ec0ca8094"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYQCsRCgH6_7"
   },
   "source": [
    "### Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AK_3M4PH7AE"
   },
   "outputs": [],
   "source": [
    "if use_saved_model:\n",
    "    # Restore the weights\n",
    "    model.load_weights(model_path) # change this\n",
    "\n",
    "    # compile the model with the specific optimizer, loss function and metric\n",
    "    model.compile(optimizer=optim, loss=loss_funct, metrics=eval_metric)\n",
    "    print(\"Weights loaded, and compiled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzHbQsZ9XG0u"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iMbKPcqTXG0w",
    "outputId": "b23d20f2-3216-4455-fbc4-fbbc1a4d0b12"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, validation_data=val_generator,\n",
    "                    validation_steps=int(np.ceil(val_data_size/batch_size_value)),\n",
    "                    steps_per_epoch=int(np.ceil(train_data_size/batch_size_value)),\n",
    "                    epochs=numEpochs, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZsJDOgaVIKY"
   },
   "source": [
    "Plot loss - IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "dBSESMffVIKb",
    "outputId": "84f0e3d9-91b7-47b5-d262-341f2d7fc633"
   },
   "outputs": [],
   "source": [
    "track_metrics = ['loss', 'psnr', 'ssim'] if loss_acronym == 'mse' else ['loss', 'jaccard_index']\n",
    "curve_name = 'loss-metric_curve_' + test_datasets + '_.png'\n",
    "create_dir(img_out_dir)\n",
    "plot_loss_and_metric(track_metrics, history, figsize=(14,7), save_fig_path=os.path.join(img_out_dir, curve_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dU96WwKr4rj_"
   },
   "source": [
    "### Save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vP11AgxF1Rd-",
    "outputId": "7596e36a-a9df-49d7-e205-9f4ad643401e"
   },
   "outputs": [],
   "source": [
    "# Save weights for future reuse\n",
    "create_dir(out_dir)\n",
    "model.save_weights( weights_filename )\n",
    "print( 'Saved model as ' + weights_filename )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEcM7GxIXG0x"
   },
   "source": [
    "## Evaluation\n",
    "\\+ display examples\n",
    "\n",
    "Method: Relevant patch with mirror padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZTyv_TKwI0q",
    "outputId": "7479500f-cdbb-4a3a-d8f8-509ec256328a"
   },
   "outputs": [],
   "source": [
    "### FULL IMAGE using relevant patch + sliding window + mirror padding\n",
    "print('TEST: ', test_datasets)\n",
    "source_test_data_path = os.path.join(data_path, test_datasets, 'test')\n",
    "test_img, test_lbl = get_xy_image_list(source_test_data_path)\n",
    "\n",
    "# Prepare the test data\n",
    "X_test = [x/255 for x in test_img] # normalize between 0 and 1\n",
    "X_test = np.expand_dims( np.asarray(X_test, dtype=np.float32), axis=-1 ) # add extra dimension\n",
    "Y_test = [x/255 for x in test_lbl] # normalize between 0 and 1\n",
    "Y_test = np.expand_dims( np.asarray(Y_test, dtype=np.float32), axis=-1 ) # add extra dimension\n",
    "\n",
    "b, h, w, c = X_test.shape\n",
    "print(\"\\nTest shape =\", X_test.shape )\n",
    "del test_img, test_lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arHQwHFB_UNY"
   },
   "source": [
    "Inferring and measuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "64f4a392040c46c29d602dea3932db15",
      "92b4abf2098146db89bd5f7520534b82",
      "8e5a3994d9e44ca083a6c95fdd3473fa",
      "cba6e66e5f394dfc9a7b04aa217c6cd5",
      "56c2a9db53e14539862c5e27bdfff820",
      "ea1151b60d864d349d156f63b77e8aca",
      "20e7cc7f64334aa2a378b99c70aeb886",
      "e80fa02b8b3943c4be6ad454f27bcc53",
      "188055d56b95475da8958466375ba489",
      "dd1cd78c7cd349489c5c102c93184a59",
      "e3f639cbe781471c82e3a50e116251c2"
     ]
    },
    "id": "UiYAFNjc_UNY",
    "outputId": "6c5af2cd-4fa9-4392-acda-3bba291bc56d"
   },
   "outputs": [],
   "source": [
    "# Now, we calculate the final test metrics\n",
    "test_iou = []\n",
    "test_psnr = []\n",
    "test_ssim = []\n",
    "test_mse = []\n",
    "preds_test = []\n",
    "input_test = []\n",
    "i = 0\n",
    "\n",
    "for img in X_test:\n",
    "\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    if w%relevant_w != 0 and h%relevant_h != 0:\n",
    "        w_parts = w/relevant_w\n",
    "        h_parts = h/relevant_h\n",
    "        new_w = int(np.ceil(w_parts))*relevant_w\n",
    "        new_h = int(np.ceil(h_parts))*relevant_h\n",
    "        pad_h = new_h - h # if pad==11 (odd): 6 top (near 0) - 5 bot\n",
    "        pad_w = new_w - w # if pad==11 (odd): 6 L   (near 0) - 5 R\n",
    "        same_shape_windows = False\n",
    "    else:\n",
    "        new_h = h + relevant_h\n",
    "        new_w = w + relevant_w\n",
    "        pad_h = relevant_h\n",
    "        pad_w = relevant_w\n",
    "        same_shape_windows = True\n",
    "\n",
    "    image = np.expand_dims(mirror_border(img[:,:,0], new_h, new_w), axis=-1)\n",
    "\n",
    "    rows = []\n",
    "    x_rows = []\n",
    "    # crete patches of (patch_h, patch_w) with (relevant_h, relevant_w) overlap between them\n",
    "    for j in range(0, image.shape[0]-relevant_h, relevant_h): \n",
    "\n",
    "        is_first_column = j == 0\n",
    "        is_last_column = j == (image.shape[0]-relevant_h*2)\n",
    "\n",
    "        columns = [] # patches of the first row\n",
    "        for k in range(0, image.shape[1]-relevant_w, relevant_w):\n",
    "            window = image[j:j + patch_h, k:k + patch_w, :]\n",
    "            columns.append( window )\n",
    "        columns = np.array(columns)\n",
    "        \n",
    "        # prepare input and gt (y)\n",
    "        if loss_acronym == 'mse':\n",
    "            y = img[:,:,0]\n",
    "            columns = np.array(columns*255, dtype='uint8')\n",
    "            columns = crappify(columns)\n",
    "            columns = np.array(columns, dtype='float32')/255\n",
    "        else:\n",
    "            y = Y_test[i,:,:,0]\n",
    "\n",
    "        _preds_test = model.predict_on_batch(columns)#, batch_size=columns.shape[0])\n",
    "\n",
    "        if same_shape_windows:\n",
    "            #all the patches contain the same padding so we can extract them directly\n",
    "            relevant_windows = _preds_test[ :, relevant_h//2 : patch_h-(relevant_h//2),\n",
    "                                            relevant_w//2 : patch_w-(relevant_w//2), :]\n",
    "\n",
    "            x_relevant = columns[ :, relevant_h//2 : patch_h-(relevant_h//2),\n",
    "                                    relevant_w//2 : patch_w-(relevant_w//2), :]\n",
    "        else:\n",
    "            # if pad==11 (odd): 5 top (near 0) - 6 bot\n",
    "            # if pad==11 (odd): 6 L   (near 0) - 5 R\n",
    "\n",
    "            # pad_h//2 padding in the top side  &&  round(pad_h/2)-1 padding in the bottom side\n",
    "            from_row = pad_h//2 if is_first_column else relevant_h//2\n",
    "            to_row = -round(pad_h/2) if is_last_column else patch_h-(relevant_h//2) \n",
    "\n",
    "            # remove especial (smaller) padding in the top and bottom side, of the first or last row\n",
    "            relevant_windows = _preds_test[ :, from_row : to_row, relevant_w//2 : patch_w - (relevant_w//2), :]\n",
    "            x_relevant = columns[ :, from_row : to_row, relevant_w//2 : patch_w - (relevant_w//2), :]\n",
    "            \n",
    "\n",
    "            # convert into list otherwise numpy raise an error due to the shape differences\n",
    "            relevant_windows = [im for im in relevant_windows]\n",
    "            x_relevant = [im for im in x_relevant]\n",
    "\n",
    "\n",
    "            # remove especial (smaller) padding in the sides\n",
    "            # the relevant window contain round(pad_w/2) size padding in the left side\n",
    "            from_column_L = round(pad_w/2)\n",
    "            to_column_L = patch_w-(relevant_w//2)\n",
    "            # the relevant window contain (pad_w//2) size padding in the right side\n",
    "            from_column_R = relevant_w//2\n",
    "            to_column_R = -(pad_w//2)\n",
    "            \n",
    "            # first column (left padding)\n",
    "            relevant_windows[0] = _preds_test[0, from_row : to_row, from_column_L : to_column_L, :]\n",
    "            x_relevant[0] = columns[0, from_row : to_row, from_column_L : to_column_L, :]\n",
    "            # last column (right padding)\n",
    "            relevant_windows[-1] = _preds_test[-1, from_row : to_row, from_column_R : to_column_R, :]\n",
    "            x_relevant[-1] = columns[-1, from_row : to_row, from_column_R : to_column_R, :]\n",
    "        \n",
    "        rows.append(cv2.hconcat(relevant_windows)) # append relevant complete row\n",
    "        x_rows.append(cv2.hconcat(x_relevant))\n",
    "\n",
    "    x_recons = cv2.vconcat(x_rows)\n",
    "    input_test.append(x_recons)\n",
    "    recons_parts = cv2.vconcat(rows)\n",
    "    preds_test.append(recons_parts) # append complete image\n",
    "\n",
    "    if loss_acronym == 'mse':\n",
    "        test_psnr.append(metrics.peak_signal_noise_ratio(recons_parts, y))\n",
    "        test_ssim.append(metrics.structural_similarity(recons_parts, y))\n",
    "        test_mse.append(metrics.mean_squared_error(recons_parts, y))\n",
    "    else:\n",
    "        test_iou.append( jaccard_index(y, recons_parts >= 0.5 ))     \n",
    "    \n",
    "    i+=1      \n",
    "    \n",
    "\n",
    "if loss_acronym == 'mse':\n",
    "    mean_psnr = np.mean(test_psnr)\n",
    "    mean_ssim = np.mean(test_ssim)\n",
    "    mean_mse = np.mean(test_mse)\n",
    "    print(\"\\nTest PSNR:\", mean_psnr)\n",
    "    print(\"\\nTest SSIM:\", mean_ssim)\n",
    "    print(\"\\nTest MSE:\", mean_mse)\n",
    "else:\n",
    "    mean_iou = np.mean(test_iou)\n",
    "    print(\"\\nTest IoU:\", mean_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Q31enQc_UNa"
   },
   "source": [
    "Show inference examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "tFRKqbZ2_UNa",
    "outputId": "0c366514-a236-4d76-ad48-b3f502f1c177"
   },
   "outputs": [],
   "source": [
    "first_pred_img_name = 'first pred plot.png'\n",
    "last_pred_img_name = 'last pred plot.png'\n",
    "\n",
    "if loss_acronym == 'mse':\n",
    "    display_list1 = [input_test[0], X_test[0,:,:,0], preds_test[0]]\n",
    "    display_list2 = [input_test[-1], X_test[-1,:,:,0], preds_test[-1]]\n",
    "    display_titles = ['Input full image', 'Ground Truth', 'Predicted image']\n",
    "else:\n",
    "    display_list1 = [X_test[0,:,:,0], Y_test[0,:,:,0], preds_test[0]>=.5, preds_test[0]]\n",
    "    display_list2 = [X_test[-1,:,:,0], Y_test[-1,:,:,0], preds_test[-1]>=.5, preds_test[-1]]\n",
    "    display_titles = ['Input Image', 'True Mask', 'Predicted Mask', 'Probability map']\n",
    "\n",
    "plt.figure(figsize=(7*4,7))\n",
    "display(display_list1,\n",
    "        custom_size = True,\n",
    "        save_fig_path = os.path.join(img_out_dir,first_pred_img_name),\n",
    "        title = display_titles,\n",
    "        )\n",
    "plt.figure(figsize=(7*4,7))\n",
    "display(display_list2,\n",
    "        custom_size = True,\n",
    "        save_fig_path = os.path.join(img_out_dir,last_pred_img_name),\n",
    "        title = display_titles,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4Ew3GyS-l_d2",
    "Yicr_LOZ_Mp9",
    "-gqnqcUzqQag",
    "BQD2El6ERR2U"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "8d589df16ddca0f3cc421f84fcbfbe02e184151fe345e9bff97bada2b58adb35"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "188055d56b95475da8958466375ba489": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "20e7cc7f64334aa2a378b99c70aeb886": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "56c2a9db53e14539862c5e27bdfff820": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64f4a392040c46c29d602dea3932db15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_92b4abf2098146db89bd5f7520534b82",
       "IPY_MODEL_8e5a3994d9e44ca083a6c95fdd3473fa",
       "IPY_MODEL_cba6e66e5f394dfc9a7b04aa217c6cd5"
      ],
      "layout": "IPY_MODEL_56c2a9db53e14539862c5e27bdfff820"
     }
    },
    "8e5a3994d9e44ca083a6c95fdd3473fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e80fa02b8b3943c4be6ad454f27bcc53",
      "max": 165,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_188055d56b95475da8958466375ba489",
      "value": 165
     }
    },
    "92b4abf2098146db89bd5f7520534b82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea1151b60d864d349d156f63b77e8aca",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_20e7cc7f64334aa2a378b99c70aeb886",
      "value": "100%"
     }
    },
    "cba6e66e5f394dfc9a7b04aa217c6cd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd1cd78c7cd349489c5c102c93184a59",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_e3f639cbe781471c82e3a50e116251c2",
      "value": " 165/165 [01:06&lt;00:00,  2.59it/s]"
     }
    },
    "dd1cd78c7cd349489c5c102c93184a59": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3f639cbe781471c82e3a50e116251c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e80fa02b8b3943c4be6ad454f27bcc53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea1151b60d864d349d156f63b77e8aca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
